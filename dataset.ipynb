{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "get ndvi"
      ],
      "metadata": {
        "id": "4oGnCusUldDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the first few metadata column names\n",
        "metadata_columns = ['Product_Name', 'MODIS_Product', 'Date', 'Coordinates', 'Identifier']\n",
        "\n",
        "# Automatically generate reflectance column names from Reflectance_1 to Reflectance_N (up to column KH)\n",
        "# We already have 5 columns for metadata, so we generate 67 additional columns (from column 6 to 72)\n",
        "reflectance_columns = [f'Reflectance_{i}' for i in range(1, 290)]  # Adjust this range if necessary\n",
        "\n",
        "# Combine metadata and reflectance columns\n",
        "columns = metadata_columns + reflectance_columns\n",
        "\n",
        "# Load the dataset, assuming it doesn't have a header\n",
        "data = pd.read_csv(r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\filtered_scaled_Nadir_Reflectance_Band2.csv\", header=None, names=columns)\n",
        "\n",
        "# Replace 'F' with NaN to handle missing/bad data\n",
        "data.replace('F', pd.NA, inplace=True)\n",
        "output_file = r'C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\band2.csv'  # Update this path\n",
        "\n",
        "# Save the updated dataset with column names to the specified CSV file location\n",
        "data.to_csv(output_file, index=False)\n",
        "\n",
        "\n",
        "# Show the first few rows of the dataset to verify column names\n",
        "print(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "sq3t1hKW7xsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get nirv"
      ],
      "metadata": {
        "id": "zEuJ_qCjlgqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the processed Band 1 and Band 2 CSV files\n",
        "# Replace these file paths with the actual processed files\n",
        "band1_data = pd.read_csv(r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\band1.csv\")  # Band 1 (Red) file path\n",
        "band2_data = pd.read_csv(r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\band2.csv\")  # Band 2 (NIR) file path\n",
        "\n",
        "# Ensure both dataframes have the same structure and are aligned properly\n",
        "\n",
        "# Extract the first five columns (metadata)\n",
        "metadata_columns = ['Product_Name', 'MODIS_Product', 'Date', 'Coordinates', 'Identifier']\n",
        "metadata = band1_data[metadata_columns]\n",
        "\n",
        "# Extract reflectance values (assumed to start from column 6 onwards)\n",
        "red_band = band1_data.iloc[:, 5:].astype(float)  # Nadir Reflectance Band 1 (Red)\n",
        "nir_band = band2_data.iloc[:, 5:].astype(float)  # Nadir Reflectance Band 2 (NIR)\n",
        "\n",
        "# Calculate NDVI: (NIR - Red) / (NIR + Red)\n",
        "ndvi = (nir_band - red_band) / (nir_band + red_band)\n",
        "\n",
        "# Calculate NIRv: NDVI * NIR\n",
        "nirv = ndvi * nir_band\n",
        "\n",
        "# Create a DataFrame for NDVI and NIRv, keeping the metadata\n",
        "final_dataset = pd.concat([metadata, ndvi.add_prefix('NDVI_'), nirv.add_prefix('NIRv_')], axis=1)\n",
        "\n",
        "# Specify the file path to save the final dataset\n",
        "output_file = r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\nirv.csv\"  # Update the path\n",
        "\n",
        "# Save the final dataset with NDVI, NIRv, and the first five columns (metadata)\n",
        "final_dataset.to_csv(output_file, index=False)\n",
        "\n",
        "# Display the first few rows of the final dataset\n",
        "print(final_dataset.head())\n",
        "\n",
        "# Notify the user about the file save location\n",
        "print(f\"File saved as {output_file}\")"
      ],
      "metadata": {
        "id": "cHLdJbU17vHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get all neccessary data together"
      ],
      "metadata": {
        "id": "ixxsboVkl0kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load DE-Kli data\n",
        "dekli_data_file = r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\FLX_DE-Kli_FLUXNET2015_SUBSET_2004-2014_1-4\\FLX_DE-Kli_FLUXNET2015_SUBSET_DD_2004-2014_1-4.csv\"  # Replace with your DE-Kli data file path\n",
        "dekli_data = pd.read_csv(dekli_data_file)\n",
        "\n",
        "# Load NDVI and NIRv reflectance data\n",
        "ndvi_nirv_data_file = r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\nirv.csv\"  # Path to your NDVI/NIRv reflectance data file\n",
        "ndvi_nirv_data = pd.read_csv(ndvi_nirv_data_file)\n",
        "\n",
        "# Convert AYDDD date format to YYYYMMDD in NDVI/NIRv data\n",
        "def convert_ayyyyddd_to_yyyymmdd(ayyyyddd):\n",
        "    year = int(ayyyyddd[1:5])\n",
        "    day_of_year = int(ayyyyddd[5:])\n",
        "    date = datetime(year, 1, 1) + timedelta(days=day_of_year - 1)\n",
        "    return date.strftime('%Y%m%d')  # Convert to YYYYMMDD format\n",
        "\n",
        "# Apply conversion to the NDVI/NIRv DataFrame\n",
        "ndvi_nirv_data['Timestamp'] = ndvi_nirv_data['Date'].apply(convert_ayyyyddd_to_yyyymmdd)\n",
        "\n",
        "# Rename NDVI and NIRv reflectance columns\n",
        "ndvi_columns = [f'NDVI_Reflectance_{m}' for m in range(1,2)]  # m from 1 to 289\n",
        "nirv_columns = [f'NIRv_Reflectance_{m}' for m in range(1, 2)]  # m from 1 to 289\n",
        "\n",
        "# Assuming NDVI and NIRv columns are named like 'NDVI_Reflectance_1', 'NDVI_Reflectance_2', ..., adjust this step if needed\n",
        "original_ndvi_columns = [col for col in ndvi_nirv_data.columns if 'NDVI_Reflectance' in col]\n",
        "original_nirv_columns = [col for col in ndvi_nirv_data.columns if 'NIRv_Reflectance' in col]\n",
        "\n",
        "# Rename NDVI columns\n",
        "ndvi_nirv_data.rename(columns=dict(zip(original_ndvi_columns, ndvi_columns)), inplace=True)\n",
        "\n",
        "# Rename NIRv columns\n",
        "ndvi_nirv_data.rename(columns=dict(zip(original_nirv_columns, nirv_columns)), inplace=True)\n",
        "\n",
        "# Ensure DE-Kli data's TIMESTAMP is a string to match\n",
        "dekli_data['Timestamp'] = dekli_data['TIMESTAMP'].astype(str)  # Adjust if the column name is different\n",
        "\n",
        "# Merge DE-Kli data with NDVI and NIRv data on the timestamp\n",
        "merged_data = pd.merge(dekli_data, ndvi_nirv_data, on='Timestamp', how='inner')\n",
        "\n",
        "# Example of selecting relevant columns\n",
        "selected_columns = [\n",
        "    'TIMESTAMP',  # DE-Kli timestamp\n",
        "    'TA_F',       # Air temperature\n",
        "    'P_F',        # Precipitation\n",
        "    'SW_IN_F',    # Shortwave radiation (or NETRAD if necessary)\n",
        "    'NEE_VUT_REF',  # Carbon flux (Net Ecosystem Exchange)\n",
        "    'VPD_F',# Vapor pressure deficit\n",
        "    'WS_F',\n",
        "    'NETRAD',\n",
        "    'G_F_MDS'\n",
        "]\n",
        "\n",
        "# Add the renamed NDVI and NIRv reflectance columns to the selected columns\n",
        "selected_columns.extend(ndvi_columns + nirv_columns)\n",
        "\n",
        "# Create the final dataset with selected columns\n",
        "final_dataset = merged_data[selected_columns]\n",
        "\n",
        "# Specify the file path to save the final dataset\n",
        "output_file = r\"C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\final.csv\"# Adjust as necessary\n",
        "\n",
        "# Save the final dataset to a new CSV file\n",
        "final_dataset.to_csv(output_file, index=False)\n",
        "\n",
        "# Display the first few rows of the final dataset\n",
        "print(final_dataset.head())\n",
        "\n",
        "# Notify the user about the file save location\n",
        "print(f\"Final dataset saved as {output_file}\")\n"
      ],
      "metadata": {
        "id": "5hH_pbt3AiaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate et"
      ],
      "metadata": {
        "id": "2OHzvxgNl43j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dokm1QB3x8h0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the merged dataset with constants\n",
        "data_file = r'C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\final.csv'  # Adjust path as needed\n",
        "data = pd.read_csv(data_file)\n",
        "\n",
        "# Constants\n",
        "gamma = 0.066  # Psychrometric constant (kPa/°C)\n",
        "Cp = 1.013 * 10**-3  # Specific heat of air (MJ/kg°C)\n",
        "rho = 1.225  # Air density (kg/m³)\n",
        "\n",
        "# Calculate Aerodynamic Conductance (Ga)\n",
        "z = 2.0  # Height of measurement (m)\n",
        "z0 = 0.1  # Roughness length for grassland (m)\n",
        "data['Ga'] = data['WS_F'] / np.log(z / z0)  # Wind speed in m/s\n",
        "\n",
        "# Calculate Saturation Vapor Pressure (e_s) and Slope (Δ)\n",
        "data['e_s'] = 6.11 * np.exp((7.5 * data['TA_F']) / (data['TA_F'] + 237.3))  # Saturation vapor pressure (kPa)\n",
        "data['Delta'] = (4098 * data['e_s']) / ((data['TA_F'] + 237.3) ** 2)  # Slope of vapor pressure curve (kPa/°C)\n",
        "\n",
        "# Calculate Surface Conductance (Gs)\n",
        "g0 = 0.1  # Minimum conductance (mol/m²/s), empirical value\n",
        "g1 = 0.5  # Sensitivity of conductance to vapor pressure deficit\n",
        "D_min = 0.1  # Minimum vapor pressure deficit (kPa)\n",
        "\n",
        "# Calculate Gs based on VPD and NDVI\n",
        "data['Gs'] = g0 + g1 * (data['VPD_F'] / D_min) * (data['NDVI'] / data['NDVI'].max())\n",
        "\n",
        "# Calculate ET using the Penman-Monteith equation\n",
        "# Assuming G (soil heat flux) is zero or from G_F_MDS column\n",
        "G = data['G_F_MDS'].fillna(0)  # Fill with 0 if not available\n",
        "\n",
        "# Calculate Vapor Pressure Deficit (D)\n",
        "data['D'] = data['VPD_F']  # Use the VPD_F column\n",
        "\n",
        "# Calculate Evapotranspiration (ET)\n",
        "data['ET'] = ((data['NETRAD'] - G) * data['Delta'] + rho * Cp * data['D'] * data['Ga']) / (data['Delta'] + gamma * (1 + data['Ga'] / data['Gs']))\n",
        "\n",
        "# Convert ET to mm/day\n",
        "data['ET_mm_per_day'] = data['ET'] / 2.45 * 86400  # Convert W/m² to mm/day\n",
        "\n",
        "# Display the first few rows with ET\n",
        "print(data[['TIMESTAMP', 'ET', 'ET_mm_per_day']].head())\n",
        "\n",
        "# Save the updated DataFrame with ET to a new CSV file\n",
        "output_file = r'C:\\Users\\melio\\OneDrive\\Desktop\\labs and lectures\\Seminar\\Datasets\\final_dataset_with_et.csv'  # Adjust path as needed\n",
        "data.to_csv(output_file, index=False)\n",
        "\n",
        "# Notify the user about the file save location\n",
        "print(f\"Updated final dataset with ET saved as {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyc63J0-lZDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
